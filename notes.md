# Chapter 2 Creating your first database and table
- sql:
    - define table structure and tables' relationship
    - query data
- Database builders prefer to organize data using separate tables for each main entity the database manages in order to reduce redundant data
- column datatype
    - `bigserial`: a special integer type that will auto-increment when a new row is added
- Text and dates (use `YYYY-DD-MM`) require quotes; numbers, including integers and decimals, don’t require quotes 
- `INSERT 0 6`: insert 6 lines, you can ignore 0 here due to legacy
- code format
    - UPPER CASE keywords and data types
    - use lowercase_and_underscores for object names, such as tables and column names 
- use DISTINCT to find bad data: e, if a school name is spelled more than one way, those spelling variations will be easy to spot and correct, especially if you sort the output; use DISTINCT on multiple columns to find distinct pairs (question: `For each x in the table, what are all the y values?`)
- Using LIKE and ILIKE with WHERE
    - ILIKE 'b%', ILIKE 'ba_er'

# Chapter 4 Understanding Data Types
- Characters
    - char, varchar, text
    ```
    # export function
    CREATE TABLE char_data_types (
 	char_column char(10),
 	varchar_column varchar(10),
 	text_column text
    );

    INSERT INTO char_data_types
    VALUES
        ('abc', 'abc', 'abc'),
        ('defghi', 'defghi', 'defghi');

    COPY char_data_types TO 'C:\Users\Lenovo\Desktop\SQL\practical-sql-2\Chapter_04\typetest.txt'
    WITH (FORMAT CSV, HEADER, DELIMITER '|');

    # result 
    char_column|varchar_column|text_column
    abc       |abc|abc
    defghi    |defghi|defghi
    ```
- Numbers
    - Integers
        - smallint(2b +/- 2**15), integer(4b), bigint(8b), if insert out of range, will have `out of range` error
            - auto-increment
                - add `GENERATED ALWAYS AS IDENTITY` (cannot insert id) Vs `GENERATED BY DEFAULT AS IDENTITY` (can still insert id)
                    - some scenarios auto-increment will create gaps in the sequence of numbers in the column. e.g. If a row is deleted, the value in that row is never replaced. Or, if a row insert is aborted (due to a bug),the sequence for the column will still be incremented.
                - special ones (similar to `GENERATED BY DEFAULT AS IDENTITY`): smallserial(2b 1~2**15), serial(4b), bigserial(8b)
    - Decimals
        - decimals(precision, scale): max digits on decimal left = precision - scale,  scale = fixed decimal digits, e.g. decimal(5, 4), then it means it will be a number less than 10 with 4 decimal digits. If u miss precision and scale tgt, it will be decimal(147455, 16383)
        - real (4b, 6 decimal) vs double precision (8b, 15 decimals): not very accurate
    - 3 rules
        - first use bigint, then use decimals (set precision large enough)

- Date
    - timestamp (8b): TIMESTAMP WITH TIME ZONE(a.k.a timestamptz)
    - date (4b)
    - time (8b)
    - interval (16b)

- Json:
    - json
    - jsonb: support indexing

- Tips:
    - expressions (computed columns): `SELECT timestamp_column - interval_column AS new_date FROM date_time_types`

# Chapter 5  Importing and Exporting Data
- postgresql doesn't use header now, so we need to exclude it when importing
- text qualifier: default as double quotes. enclose a column that includes the delimiter character
    - if Postgresql finds 2 consecutive text qualifiers inside a double-quoted column, it will remove one
    ```sql
    CREATE TABLE us_countries_2010 (
        geo_name VARCHAR(90),
        state_us_abbreviation VARCHAR(2),
        ...
    ) 

    COPY us_countries_2010 (geo_name, ...)
    FROM 'C:\..'
    WITH (FORMAT CSV, HEADER, DELIMITER ',')
    WHERE state = 'Alaska';
    ```
- [primary key](https://www.postgresql.org/docs/current/ddl-constraints.html#DDL-CONSTRAINTS-PRIMARY-KEYS): A primary key constraint indicates that a column, or group of columns, can be used as a **unique and not null** identifier for rows in the table. A table can have at most one primary key. or example, a GUI application that allows modifying row values probably needs to know the primary key of a table to be able to identify rows uniquely. There are also various ways in which the database system makes use of a primary key if one has been declared; for example, the primary key defines the default target column(s) for foreign keys referencing its table.
- you can delete the content of table by ```DELETE FROM supervisor_salaries```, but this wont reset the auto-increment id columns. If you want to drop the table, use ```DROP TABLEs```
- temporary table: exists only during current connection session. used to perform intermediary operations for data pipeline
```sql
CREATE TEMPORARY TABLE supervisor_salaries_temp
    (LIKE supervisor_salaries INCLUDING ALL);  
-- INCLUDING ALL means include index and indentities
COPY supervisor_salaries_temp (town, supervisor, salary)
FROM `C:\...`
WITH (FORMAT CSV, HEADER);
INSERT INTO supervisor_salaries (town, country, supervisor, salary)
    SELECT town, 'Millis', supervisor, salary 
    FROM supervisor_salaries_temp
DROP TABLE supervisor_salaries_temp
```

# Chapter 6 Basic Math and Stats with SQL
- With PostgreSQL, you can omit the table name and perform simple math and string operations using SELECT. e.g SELECT 1 + 1
 - array constructor and median
 ```sql
 SELECT percentile_count(ARRAY[0.25, 0.5, 0.75])
 WITHIN GROUP(ORDER BY pop_est_2019) AS quantiles
 FROM us_countires_2019;
 ```
- foreign key: require its value already exists in the column it references. can refer to any column with unique value
- why store in several database?
    - save space by not repeating lengthy strings
    - easier to manage, can only change one part
- filter NULL: WHERE col IS (NOT) NULL
- concatenate methods
    - UNION (unique only) Vs UNION ALL (support duplicate)
    - INTERSECT
    - EXCEPT

# Chapter 8 Table Design that works for you
- Naming Convention
    - use plural noun for table names
    - use snake name
- Constraints
    - 2 types:
        - table constraint
            - `CONSTRAINT {constraint_name} {SQL CONSTRAINT KEYWORD} [table_name](column, [,..])`
        - column constraint
    - tips
        - use GENERATED ALWAYS AS IDENTITY keyword to a column so that the column 1) does not need to insert value 2) will auto insert value   
        ```sql
        <!-- OVERRIDE IDENTITY COLUMN -->
        INSERT INTO surrogate_key_example 
        OVERRIDING SYSTEM VALUE 
        VALUES (4, 'Chicken Coop', '2021-09-03 10:33-07'); 
        
        <!-- OVERRIDE IDENTITY COLUMN START NUMBER -->
        ALTER TABLE surrogate_key_example ALTER COLUMN order_number 
        RESTART WITH 5; 
        
        <!-- NORMAL USE -->
        INSERT INTO surrogate_key_example (product_name, order_time) 
        VALUES ('Aloe Plant', '2020-03-15 10:09-07');
        ```
    - several constraints
        - PRIMARY KEY
        - FOREIGN KEY: when you add foreign key, the primary key it references must exist in the reference table. when you delete primary key, you also need to delete all its corresponding foreign keys. (use ON DELETE CASCADE to do so)
        ```sql
         CREATE TABLE licenses ( 
            license_id text, 
            first_name text, 
            last_name text, 
         CONSTRAINT licenses_key PRIMARY KEY (license_id) 
        ); 
        <!-- Deleting a row in licenses should also delete all related rows in -->
        <!-- registrations. This allows us to delete a driver’s license without first -->
        <!-- having to manually remove any registrations linked to it. It also maintains -->
        <!-- data integrity by ensuring deleting a license doesn’t leave orphaned rows in -->
        <!-- registrations -->
        CREATE TABLE registrations ( 
            registration_id text, 
            registration_date timestamp with time zone, 
            license_id text REFERENCES licenses (license_id) ON DELETE CASCADE, 
            CONSTRAINT registration_key PRIMARY KEY (registration_id, 
        license_id) 
        ); 
        ```
        - CHECK
        ```sql
        CREATE TABLE check_constraint_example ( 
            user_id bigint GENERATED ALWAYS AS IDENTITY, 
            user_role text, 
            salary numeric(10,2), 
            CONSTRAINT user_id_key PRIMARY KEY (user_id), 
            CONSTRAINT check_role_in_list CHECK (user_role IN('Admin', 
        'Staff')), 
            CONSTRAINT check_salary_not_below_zero CHECK (salary >= 0 AND salary<=1000000000>) 
        );
        ```
        - UNIQUE: compared to PRIMARY KEY, UNIQUE column can accept NULL
        - NOT NULL
    - ADD OR REMOVE constraints
    ```sql
    ALTER TABLE not_null_example DROP CONSTRAINT student_id_key; 
    ALTER TABLE not_null_example ADD CONSTRAINT student_id_key PRIMARY KEY (student_id); 
    ALTER TABLE not_null_example ALTER COLUMN first_name DROP NOT NULL; 
    ALTER TABLE not_null_example ALTER COLUMN first_name SET NOT NULL;
    ```
- (B-tree) index: can be applied to multiple columns. a seperate data structure that the database manages
    - how to add?
        - `PRIMARY KEY` or `UNIQUE` will auto create index
        - `CREATE INDEX {index name} ON {table_name} ({column_name}[,...])`
    - useful for data that can be ordered and searched using equality and range and LIKE if there is no wildcard at the beginning of the string (e.g. `WHERE chips LIKE 'Dorito%'`)
    ```sql
    -- Listing 8-13: Benchmark queries for index performance using EXPLAIN ANALYZE
    EXPLAIN ANALYZE SELECT * FROM new_york_addresses
    WHERE street = 'BROADWAY';

    CREATE INDEX street_idx ON new_york_addresses (street);
    DROP INDEX street_idx;
    ```
    - what columns to use?
        - Foreign Keys (speed up join and delete cascade)
        - columns you’ll use in table joins
        - Add indexes to columns that will frequently end up in a query WHERE clause.
 
 # Chapter 9 Extracting Information By Group and Summarizing
 - count
    - count(*): return row numbers, **include NULL**
    - count({colname}): return **NOT NULL** values in a column
    - count(DISTINCT {colname}): return **distinct NOT NULL values** in a column
- GROUP BY (== DISTINCT)
- tips:
    - first `WHERE` then `JOIN` then `GROUP BY` then `SELECT` then `ORDER BY`
- HAVING Vs WHERE
    - aggregate functions, such as sum(), can’t be used within a WHERE clause because they operate at the row level, and aggregate functions work across rows. The HAVING clause places conditions on groups created by aggregating
    - **HAVING wil filter out the whole group rather than some records**

# Chapter 10 Inspecting and Modifying Data
- Data interview, e.g. (use GROUP BY)
    - missing data
    - misspelling data
- Modify Table, Column and Data
    - command 1: start with `ALTER TABLE`, then paired with `ADD COLUMN`, `ALTER COLUMN`, `DROP COLUMN`
    ```sql
    ALTER TABLE table ADD COLUMN column data_type;
    ALTER TABLE table DROP COLUMN column;
    ALTER TABLE table ALTER COLUMN column SET DATA TYPE data_type;
    ALTER TABLE table ALTER COLUMN column SET NOT NULL;
    ALTER TABLE table ALTER COLUMN column DROP NOT NULL;
    ```
    - command 2: start with `UPDATE`, then paired with `WHERE`
    ```sql
    -- set value could be column_name 
    UPDATE table 
    SET column_a = value_a, column_b = value_b 
    WHERE criteria;

    -- DO NOT UNDERSTAND: UPDATE table 
    UPDATE table 
    SET column = (SELECT column 
                  FROM table_b 
                  WHERE table.column = table_b.column) 
    WHERE EXISTS (SELECT column 
                  FROM table_b 
                  WHERE table.column = table_b.column);
    
    ALTER TABLE meat_POULTRY_EGG_ESTABLISHMENTS
    ADD COLUMN inspection_deadline timestamp with time zone

    -- WHERE EXISTS and WHERE here is like join, help to update the value across tables    
    UPDATE meat_poultry_egg_establishments establishments
    SET inspection_deadline = '2022-12-01 00:00 EST'
    WHERE EXISTS (SELECT state_regions.region FROM state_regions WHERE establishments.st = state_regions.st AND state_regions.region='NEW ENGLAND');

    ```
    - Viewing Modified Data with RETURNING, can be paired with `UPDATE`, `INSERT`, `DELETE FROM`
    ```sql
    UPDATE table 
    SET column_a = value 
    RETURNING column_a, column_b, column_c;
    ```
    - Create backup tables:  Indexes are not copied when creating a table backup using the
    CREATE TABLE statement. If you decide to run queries on the backup, be sure to create a separate index on that table.
    ```sql
    CREATE TABLE meat_poultry_egg_establishments_backup AS SELECT * FROM meat_poultry_egg_establishments;
    ```
    - tips:
        - `IS DISTINCT FROM`(treat NULL as known and will only return true/false) Vs `<>` (a comparison that includes a NULL will return NULL)
- Delete data
    - can delete table/database/rows/columns
        - rows: `DELETE FROM table_name WHERE expressions;` or `TRUNCATE table_name RESTART IDENTITY`
        - columns: `ALTER TABLE table_name DROP COLUMN column_name`
        - table: `DROP TABLE table_name`
- Use [transaction](https://www.postgresql.org/docs/current/tutorial-transactions.html) to save or revert changes (atomic operation)
    - you can run by step inside the transaction block, but none of your changes will be visible to other database until you commit or rollback
    ```sql
    START TRANSACTION
    UPDATE meat_poultry_egg_establishments
    SET company='ARGO Merchants Oakland LLC'
    WHERE company='ARGP Merchants Oakland, LLC';

    SELECT company
    FROM meat_poultry_egg_establishments
    WHERE company LIKE 'ARGP%'
    ORDER BY company;
    ROLLBACK;
    ```
- Improving performance when updating large tables
    - when you add new columns, you should copy to a new table and rename the table; if you only add column, then it will create a new version of existing row each time a value is updated, but it does not delete the old version, thus the table size will increase (FIXME: confusing)

# Chapter 11 Statistical functions in SQL
- corr, regr_slope, regr_intercept, regr_r2, var_pop, var_samp, stddev_pop, stddev_samp
- window functions: perform calculations across a set of rows relative to current row, using `OVER ([PARITION BY col] ORDER BY col [ROWS BETWEEN n PRECEDING AND CURRENT ROW])`
    - rank: generate gap when tie
    - dense_rank: no gap when tie
    ```sql
    SELECT category, store, unit_sales, rank() OVER (PARTITION BY category ORDER BY unit_sales DESC);
    ```
    - rolling window
    ```sql
    SELECT year, month, citrus_export_value, round(avg(citrus_export_value) OVER (ORDER BY year, month ROWS BETWEEN 11 PRECEDING AND CURRENT ROW), 0) AS twelve_month_avg
    FROM us_exports
    ORDER BY year, month;
    ```

# Chapter 12 Working with Dates and Times
- system database pg_timezone_names: (used for checking timezone)
    - `SELECT * FROM pg_timezone_names ORDER BY name;`
- set time zone for session 
    - `SET TIME ZONE 'US/Pacific';`
    - `SELECT test_date AT TIME ZONE 'Asia/Seoul' FROM time_zone_test`;
- cumsum 
```sql
SELECT segment, 
       arrival - departure AS segment_duration, 
       sum(arrival - departure) OVER (ORDER BY trip_id) AS 
cume_duration 
FROM train_rides;
-- OVER (ORDER BY trip_id ROWS BETWEEN TOP AND CURRENT ROW)
```

# Chapter 13 ADVANCED QUERY TECHNIQUES
- place scalar return subquery to WHERE clause
- place rows/columns return subquery to FROM clause to create a *derived table*
- (subquery expression)[https://www.postgresql.org/docs/current/functions-subquery.html]:  a combination of a keyword with a subquery and are generally used in WHERE clauses to filter rows based on the existence of values in another table
    - `IN (subquery)`: column must be in the array returned by subquery
        - Avoid using `NOT IN`. The presence of `NULL` values in a subquery result set will cause a query with a `NOT IN` expression to return no rows. The PostgreSQL wiki recommends using `NOT EXISTS` instead
    -  `EXISTS (subquery)` returns a value of true if the subquery in parentheses returns at least one row. If it returns no rows, EXISTS evaluates to false
        - if the subquery is correlated,it will **execute once for each row returned by the outer query**, each time checking whether there’s an id in retirees that matches emp_id in employees. If there is a match, the EXISTS expression returns true.
    - `FROM table_name, LATERAL (subquery) alias`:  Subqueries appearing in FROM can be preceded by the key word LATERAL. This allows them to reference columns provided by preceding FROM items.
    - `JOIN LATERAL (subquery) alias`: for each row generated by the query in front of the LATERAL join, a subquery or function after the LATERAL join will be evaluated once
- common table expression (a.k.a `WITH`quiries): use one or more SELECT queries to predefine temporary tables taht you can reference. 
```sql
WITH
    counties (st, pop_est_2018) AS
    (SELECT state_name, sum(pop_est_2018)
     FROM us_counties_pop_est_2019
     GROUP BY state_name),

    establishments (st, establishment_count) AS
    (SELECT st, sum(establishments) AS establishment_count
     FROM cbp_naics_72_establishments
     GROUP BY st)

SELECT counties.st,
       pop_est_2018,
       establishment_count,
       round((establishments.establishment_count / 
              counties.pop_est_2018::numeric(10,1)) * 1000, 1)
           AS estabs_per_thousand
FROM counties JOIN establishments
ON counties.st = establishments.st
ORDER BY estabs_per_thousand DESC;
```
- TODO: (recursive)[https://www.postgresql.org/docs/current/queries-with.html#QUERIES-WITH-RECURSIVE]
- Perform Pivot Table
```sql
-- First install module by below:
-- CREATE EXTENSION tablefunc

SELECT *
FROM crosstab('SELECT
                  station_name,
                  date_part(''month'', observation_date),
                  percentile_cont(.5)
                      WITHIN GROUP (ORDER BY max_temp)
               FROM temperature_readings
               GROUP BY station_name,
                        date_part(''month'', observation_date)
               ORDER BY station_name',

              'SELECT month
               FROM generate_series(1,12) month')
-- generate_series(1, 12) month: generate 1~12 series with column named as month

AS (station text,
    jan numeric(3,0),
    feb numeric(3,0),
    mar numeric(3,0),
    apr numeric(3,0),
    may numeric(3,0),
    jun numeric(3,0),
    jul numeric(3,0),
    aug numeric(3,0),
    sep numeric(3,0),
    oct numeric(3,0),
    nov numeric(3,0),
    dec numeric(3,0)
);
```
- Reclassify values with CASE
```sql
CASE WHEN condition THEN result
    WHEN another_condition THEN result
    ELSE result
END

-- Without an ELSE clause, the statement would return a NULL when no conditions are true
```

# Chapter 14 Mining Text to find meaning data
- `WHERE ~*`: where case insensitive regex filter; `WHERE ~`: where case sensitive regex filter; `WHERE !~*`: where case insensitive regex anti-filter 
- regex expression: extract data from each row, `regexp_match(colname, regex_pattern)`
```sql
UPDATE crime_reports
SET date_1 = 
    (
      (regexp_match(original_text, '\d{1,2}\/\d{1,2}\/\d{2}'))[1]
          || ' ' ||
      (regexp_match(original_text, '\/\d{2}\n(\d{4})'))[1] 
          ||' US/Eastern'
    )::timestamptz,
             
    date_2 = 
    CASE 
    -- if there is no second date but there is a second hour
        WHEN (SELECT regexp_match(original_text, '-(\d{1,2}\/\d{1,2}\/\d{2})') IS NULL)
                     AND (SELECT regexp_match(original_text, '\/\d{2}\n\d{4}-(\d{4})') IS NOT NULL)
        THEN 
          ((regexp_match(original_text, '\d{1,2}\/\d{1,2}\/\d{2}'))[1]
              || ' ' ||
          (regexp_match(original_text, '\/\d{2}\n\d{4}-(\d{4})'))[1] 
              ||' US/Eastern'
          )::timestamptz 

    -- if there is both a second date and second hour
        WHEN (SELECT regexp_match(original_text, '-(\d{1,2}\/\d{1,2}\/\d{2})') IS NOT NULL)
              AND (SELECT regexp_match(original_text, '\/\d{2}\n\d{4}-(\d{4})') IS NOT NULL)
        THEN 
          ((regexp_match(original_text, '-(\d{1,2}\/\d{1,2}\/\d{2})'))[1]
              || ' ' ||
          (regexp_match(original_text, '\/\d{2}\n\d{4}-(\d{4})'))[1] 
              ||' US/Eastern'
          )::timestamptz 
    END,
    street = (regexp_match(original_text, 'hrs.\n(\d+ .+(?:Sq.|Plz.|Dr.|Ter.|Rd.))'))[1],
    city = (regexp_match(original_text,
                           '(?:Sq.|Plz.|Dr.|Ter.|Rd.)\n(\w+ \w+|\w+)\n'))[1],
    crime_type = (regexp_match(original_text, '\n(?:\w+ \w+|\w+)\n(.*):'))[1],
    description = (regexp_match(original_text, ':\s(.+)(?:C0|SO)'))[1],
    case_number = (regexp_match(original_text, '(?:C0|SO)[0-9]+'))[1];
```
- full-text search in postgresql
    - text search data types: you will use `tsquery` to search `tsvector`
        - `tsvector`: represents the text to be searched and stored in a sorted list of *lexemes*(linguistic unit in a given language, a.k.a word root).
            - For example, a tsvector type column would store the words washes, washed, and washing as the lexeme wash while noting each word’s position in the original text.
            - converting text to tsvector `to_tsvector(language, string)` also moves *stop word*, e.g. the or it
            - use `GIN` index on `tsvector` column
            ```sql
            CREATE INDEX search_idx ON president_speeches USING gin(search_speech_text);
            ```
        - `tsquery`: converting text to tsquery `to_tsquery(language, string)`, represents the search query terms and operators 
        - how to use `tsquery` to search `tsvector`
            - filter tsquery: `tsvector @@ ts_query` (tsvector include tsquery)
            ```sql
            SELECT president, speech_date 
            FROM president_speeches 
            WHERE search_speech_text @@ to_tsquery('english', 'Vietnam') 
            ORDER BY speech_date;
            ```

            - show search result location: `ts_headline(string column not the tsquery column, tsquery, option)`
            ```sql
            -- options: enclose the matched word with "<>", display 5~7 words, and show max 1 match
            SELECT president, 
            speech_date, 
	        ts_headline(speech_text, to_tsquery('english', 'tax'), 
				   'StartSel = <, 
                    StopSel = >, 
                    MinWords=5, 
                    MaxWords=7, 
                    MaxFragments=1') 
            FROM president_speeches 
            WHERE search_speech_text @@ to_tsquery('english', 'tax') 
            ORDER BY speech_date;
            ```
            - find consecutive words <n>
            ```sql
            -- find pattern(defense after 1-1=0 word is military)
            --  If you changed the query terms to military <2> defense, the database would return matches where the terms are exactly two words apart, as in the phrase “our military and defense commitments.”
            SELECT president, 
            speech_date, 
            ts_headline(speech_text, 
                   to_tsquery('english', 'military <1> defense'), 
                   'StartSel = <, 
                    StopSel = >, 
                    MinWords=5, 
                    MaxWords=7, 
                    MaxFragments=1') 
            FROM president_speeches 
            WHERE search_speech_text @@ 
                  to_tsquery('english', 'military <1> defense') 
            ORDER BY speech_date;
            ```

            - rank on how often the lexemes you’re searching for appear in the text (can be normalized by dividing the sentence length)
            ```sql
            -- ts_rank(tsvector, tsquery[,2]): consider frequencies only, optional with consideration of length of sentence
            -- ts_rank_cd(tsvector, tsquery[,2]): consider frequencies, density only, optional with consideration of length of sentence
            SELECT president,
            speech_date,
            ts_rank(search_speech_text,
               to_tsquery('english', 'war & security & threat & enemy'))
               AS score
            FROM president_speeches
            WHERE search_speech_text @@ 
                  to_tsquery('english', 'war & security & threat & enemy')
            ORDER BY score DESC
            LIMIT 5;
            ```

# Chapter 16 Working with JSON data
- JSON: comprises 2 data structure that could be permutated together
    - **object**: an unordered set of name/value pairs delimited by comma 
    - **array**: an ordered collection of values (values could be **object**)
- Data types: both allow insertion of valid JSON-only text (enclosed by curly brackets, common seperating objects, proper quoting of keys)
    - json: slow, used only for duplicate keys and preserving the order of keys
    - jsonb: [recommended]much much faster, preserve only the last key/value pair for duplicate keys, order of keys are not preserved, 
    ```sql
    -- import json
     CREATE TABLE films ( 
        id integer GENERATED ALWAYS AS IDENTITY PRIMARY KEY, 
        film jsonb NOT NULL 
    ); 
    COPY films (film) 
    FROM C:\YourDirectory\films.json'; 
    CREATE INDEX idx_film ON films USING GIN (film);
    ```
- Extraction
    - key/value extraction: ->key returns jsonb / ->>key returns text
    - array element extraction:  ->integer returns jsonb / -->integer returns text
    - path extraction
        - path: a series of keys or array indices that lead to the location of a value. #> '{key/integer[,key/integer]}' returns jsonb, #>> '{key/integer[,key/integer]}' returns text
        ```sql
        SELECT id, film #> '{characters, 0, name}' AS name 
        FROM films 
        ORDER BY id; 
        SELECT id, film #>> '{characters, 0, name}' AS name 
        FROM films 
        ORDER BY id;
        ```
- Containment: jsonb @> jsonb (or `{key:value}::jsonb`) returns boolean
    ```sql
    SELECT id, film ->> 'title' AS title, 
       film @> '{"title": "The Incredibles"}'::jsonb AS is_incredible 
    FROM films 
    ORDER BY id;
    ```
- Existence: jsonb [? text | ?| text[] | ?& text[]], whether jsonb includes key text, include any of text[], include all keys
- Convert to json: turn a normal table into a table with one column consisting of json lines using `to_json(table_name)` and then turn a table with one column consisting of json lines to one json line using `json_agg(table)`
    ```sql
    -- normal table to json table
    SELECT to_json(employees) AS json_rows 
    FROM ( 
    SELECT emp_id, last_name AS ln2 FROM employees 
    ) AS employees;

    -- json table to json string
    SELECT json_agg(to_json(employees)) AS json_rows 
    FROM ( 
    SELECT emp_id, last_name AS ln2 FROM employees 
    ) AS employees;
    ```
- CRUD
    ```sql
    -- add
    UPDATE films 
    SET film = film || '{"studio": "Pixar"}'::jsonb 
    WHERE film @> '{"title": "The Incredibles"}'::jsonb; 

    -- update
    UPDATE films 
    SET film = jsonb_set(film, 
                 '{genre}', 
                  film #> '{genre}' || '["World War II"]',  
                  true) 
    WHERE film @> '{"title": "Cinema Paradiso"}'::jsonb;

    -- delete key by -
    UPDATE films 
    SET film = film - 'studio' 
    WHERE film @> '{"title": "The Incredibles"}'::jsonb; 
    
    -- delete element by using path operator #-
    UPDATE films 
    SET film = film #- '{genre, 2}' 
    WHERE film @> '{"title": "Cinema Paradiso"}'::jsonb;
    ```
- 3 useful json processing function:
    - `jsonb_array_length`: get the length of an array in json
    - `jsonb_array_elements` or `jsonb_array_elements_text`: convert array elements into rows, with one row per element

# Chapter 17 Saving time with VIEWS, FUNCTIONS and TRIGGERS
- View: a stored query with a name that you can work with as if it were a table
    - reusable view: Every time you access a standard view, the stored query runs and generates a **temporary** set of results
        - if you’re replacing an existing view, the new query must generate the same column names with the same data types and in the same order as the one it’s replacing. You can add columns, but they must be placed at the end of the column list. If you try to do otherwise, the database will respond with an error message.
        ```sql
        CREATE OR REPLACE VIEW county_pop_change_2019_2010 AS
            SELECT c2019.county_name,
                   c2019.state_name,
                   c2019.state_fips,
                   c2019.county_fips,
                   c2019.pop_est_2019 AS pop_2019,
                   c2010.estimates_base_2010 AS pop_2010,
                   round( (c2019.pop_est_2019::numeric - c2010.estimates_base_2010)
                       / c2010.estimates_base_2010 * 100, 1 ) AS pct_change_2019_2010
            FROM us_counties_pop_est_2019 AS c2019
                JOIN us_counties_pop_est_2010 AS c2010
            ON c2019.state_fips = c2010.state_fips
                AND c2019.county_fips = c2010.county_fips;
        ```
        - you can limit access to CRUD by using non-materialized view
        ```sql
         CREATE OR REPLACE VIEW employees_tax_dept WITH (security_barrier) AS 
            SELECT emp_id, 
                   first_name, 
                   last_name, 
                   dept_id 
            FROM employees 
            WHERE dept_id = 1 
            WITH LOCAL CHECK OPTION;
        -- WITH (security_barrier): This enables a level of database security to prevent a malicious user from getting around restrictions that the view places on rows and columns (https://www.postgresql.org/docs/current/rules-privileges.html)
        -- WITH LOCAL CHECK OPTION: restricts inserts per WHERE. in this case, it restricts the insertion's dept_id=1
        ```

    - materialized view: Every time you access a standard view, the **permanently** stored view shows. If update is needed, need to refresh it
        - A good use for materialized views is to preprocess complex queries that take a while to run and make those results available for faster querying
        ```sql
        -- create materialzied view
        CREATE MATERIALIZED VIEW nevada_counties_pop_2019 AS 
            SELECT county_name, 
                   state_fips, 
                   county_fips, 
                   pop_est_2019 
            FROM us_counties_pop_est_2019 
            WHERE state_name = 'Nevada';

        -- refresh materialized view    
        REFRESH MATERIALIZED VIEW nevada_counties_pop_2019;
    ```
- Functions / Procedures: can define by sql or precedual language such as PL/pgSQL or PL/Python, you can use pyAgent to schedule a procedure
    ```sql
    -- function
    CREATE OR REPLACE FUNCTION percent_change(
	    new_value numeric,
	    old_value numeric,
	    decimal_places integer DEFAULT 1
    ) RETURNS numeric AS 
    $$
    SELECT round((new_value - old_value) / old_value * 100, decimal_places);
    $$
    LANGUAGE SQL
    IMMUTABLE
    RETURNS NULL ON NULL INPUT;

    -- procedure
    -- enclose procedure code in $$. BEGIN...END is like {...} in C#. RAISE NOTICE is like print
    CREATE OR REPLACE PROCEDURE update_personal_days() AS 
    $$
    BEGIN
    	UPDATE teachers 
    	SET personal_days =
    		CASE WHEN (now() - hire_date) >= '10 years'::interval
    			AND (now() - hire_date) < '15 years'::interval
    			THEN 4
    		WHEN (now() - hire_date) >= '15 years'::interval
    			AND (now()-hire_date) < '20 years'::interval
    			THEN 5
    		WHEN (now() - hire_date) >= '20 years'::interval
    			AND (now() - hire_date) < '25 years'::interval
    			THEN 6
    		WHEN (now()-hire_date) >= '25 years'::interval 
    			THEN 7
    		ELSE 3
    		END;
    		RAISE NOTICE 'personal_days updates!';
    END;
    $$
    LANGUAGE plpgsql;

    CALL update_personal_days();

    -- using Python
    -- debug: plpython3.dll cannot load --> add C:\edb\languagepack\v2\Python-3.9 to PATH and create PYTHONHOME=C:\edb\languagepack\v2\Python-3.9 --> restart PC!! Do it otherwise would not work 😊
    CREATE EXTENSION plpython3u;

    CREATE OR REPLACE FUNCTION trim_county(input_string text) 
    RETURNS text AS 
    $$ 
        import re
        cleaned = re.sub(r' County', '', input_string) 
        return cleaned 
    $$ 
    LANGUAGE plpython3u;

    SELECT county_name, 
       trim_county(county_name) 
    FROM us_counties_pop_est_2019 
    ORDER BY state_fips, county_fips 
    LIMIT 5;
    ```
- Triggers: executes a function whenever a specified event, such as an INSERT, UPDATE, or DELETE, occurs on a table or a view. You can set a trigger to fire before, after, or instead of the event, and you can also set it to fire once for each row affected by the event or just once per operation. e.g. You could set the trigger to fire once for each of the 20 rows deleted or just one time
    - add loggings **AFTER UPDATE**
    ```sql
    -- First, create trigger function
    -- in the function. The first is the row values before they were changed, noted with the prefix OLD. The second is the row values after they were changed, noted with the prefix NEW. 
    -- in the function, you need to RETURNS trigger and RETURNS NULL
    CREATE OR REPLACE FUNCTION record_if_grade_changed() 
    RETURNS trigger AS 
    $$ 
    BEGIN 
      IF NEW.grade <> OLD.grade THEN 
        INSERT INTO grades_history ( 
            student_id, 
            course_id, 
            change_time, 
            course, 
            old_grade, 
            new_grade) 
        VALUES 
            (OLD.student_id, 
             OLD.course_id, 
             now(), 
             OLD.course, 
      		 OLD.grade, 
      		 NEW.grade); 
      END IF; 
      RETURN NULL; 
     END; 
    $$ 
    LANGUAGE plpgsql;

    -- Second, create trigger rule
    CREATE TRIGGER grades_update
	AFTER UPDATE ON grades
	FOR EACH ROW 
	EXECUTE PROCEDURE if_grade_changed();
    ```
    - update before inserting 
    ```sql
    -- plpython doc: https://www.postgresql.org/docs/current/plpython-trigger.html
    CREATE OR REPLACE FUNCTION classify_max_temp()
        RETURNS TRIGGER AS
    $$
        max_temp = TD["new"]["max_temp"]
        if max_temp is not None:
            if max_temp >= 90:
                TD["new"]["max_temp_group"] = 'Hot'
            elif 70 <= max_temp < 90:
                TD["new"]["max_temp_group"] = 'Warm'
            elif 50 <= max_temp < 70:
                TD["new"]["max_temp_group"] = 'Pleasant'
            elif 33 <= max_temp < 50:
                TD["new"]["max_temp_group"] = 'Cold'
            elif 20 <= max_temp < 33:
                TD["new"]["max_temp_group"] = 'Frigid'
            elif max_temp < 20:
                TD["new"]["max_temp_group"] = 'Inhumane'
            else:
                TD["new"]["max_temp_group"] = 'No reading'
        else:
            TD["new"]["max_temp_group"] = 'No reading'
        return "MODIFY"

    $$ LANGUAGE plpython3u;

    -- before insert add value for another column
    CREATE TRIGGER temperature_insert 
    BEFORE INSERT 
      ON temperature_test 
    FOR EACH ROW 
    EXECUTE PROCEDURE classify_max_temp();

    INSERT INTO temperature_test
    VALUES
        ('North Station', '1/19/2023', 10, -3),
        ('North Station', '3/20/2023', 28, 19),
        ('North Station', '5/2/2023', 65, 42),
        ('North Station', '8/9/2023', 93, 74),
        ('North Station', '12/14/2023', NULL, NULL);
	
    table temperature_test;
    ```

# Chapter 18 Command Line - psql
- `psql -d database_name -U user_name -h host_domain`
- useful [meta-commands](https://www.postgresql.org/docs/current/app-psql.html#APP-PSQL-META-COMMANDS) 
    - `\dt`: show all table in a database
    - `\du`: show all users
    - `\dv`: show all views
    - `\l`: show all databases
    - `\dx`: show all extensions
    - `\d table_name`: show table schema (columns)
    - `\df`: show all functions
    - `\c database_name`
    - `\e` edit last query
    - `\x`: extended view, show each row in a list

- save password for connection to a database
    -  enter `hostname:port:database:username:password` to `%APPDATA\postgresql\pgpass.conf`, e.g. ` localhost:5432:*:postgres:password`
- 
- save query output to a file
    ```sql
    -- open the file for appending query results below
    \o 'C:/YourDirectory/query_output.csv'

    query1
    query2
    query3

    -- close the file, following queries will be output to console
    \o
    ```
- read and execute sql stored in a file
    ```sql
     psql -d analysis -U postgres -f C:\YourDirectory\display-grades.sql
    ```

# Chapter 19 Maintaining your database
- check table size: `\dt+ table_name`; `SELECT pg_size_pretty(pg_total_relation_size('vacuum_test'));`
    - for every updated value, PostgreSQL creates a new row, and the dead row remains in the table. Even though you see only 500,000 rows, the table has double that number
    - `VACUUM table_name`(will tag the dead rows as places to be reused, but wont delete) Vs `VACUUM FULL table_name`(will delete the dead rows)
- [statistic collector](https://www.postgresql.org/docs/current/monitoring-stats.html) (views that track database activilty and usage): `SELECT relname, last_vacuum, last_autovacuum, vacuum_count, autovacuum_count FROM pg_stat_all_tables WHERE relname='vacuum_test'`
- Export and Import Database
    - `pg_dump -d analysis -U [user_name] -Fc -v -f analysis_backup.dump`
    - `pg_restore -C -d postgres -U postgres analysis_backup.dump`